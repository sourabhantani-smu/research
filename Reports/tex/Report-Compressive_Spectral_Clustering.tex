\documentclass[10pt,a4paper, nocenter]{report}
\usepackage[scaled=0.92]{helvet}
\usepackage[margin=1in]{geometry}
\usepackage[latin1]{inputenc}
\usepackage{blindtext}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb,amsthm}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{enumerate,url}
\usepackage{etoolbox}
\usepackage{algorithm2e}

\makeatletter
\patchcmd{\chapter}{\if@openright\cleardoublepage\else\clearpage\fi}{}{}{}
\makeatother
\makeatletter
\patchcmd{\chapter}{\@maketitle\cleardoublepage}{}{}{}
\makeatother

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]


\newtheorem{prop}{Proposition}
\newtheorem{theorem}{Theorem}



\newcommand{\abs}[1]{\lvert {#1} \rvert}
\newcommand{\norm}[1]{\lvert\lvert {#1} \rvert\rvert}


\pagestyle{fancy}
\lhead{[WORK IN PROGRESS] \\ Report on Paper "Compressive Spectral Clustering" \\by Tremblay, Puy, Gribnoval, Vandergheynst}
\rhead{Sourabh Antani}
\cfoot{\thepage\ of \pageref{LastPage}}
\renewcommand{\footrulewidth}{0.4pt}


\author{Sourabh Antani}
\title{[WORK IN PROGRESS] \\ Report on Paper "Compressive Spectral Clustering" \\by Nicolas Tremblay, Gilles Puy, R$\acute{e}$mi Gribnoval, Pierre Vandergheynst}
\date{}

\begin{document}
	\maketitle

	
	\chapter{Introduction}
	\thispagestyle{fancy}
	This is a paper report on the paper 'Compressive Spectral Clustering' by by Nicolas Tremblay, Gilles Puy, R\'{e}mi Gribnoval and Pierre Vandergheynst \cite{tremblay-compressive-SC-16}. All the sections in this report are taken or paraphrased from \cite{tremblay-compressive-SC-16} and not original work. The structure of this report also follows the structure of the paper so that it is easy to follow.

	According to the authors, Spectral Clustering has three computational bottlenecks when $N$ and/or $k$ is large: Creation of the similarity matrix $W$, partial eigendecomposition of $L$ and $k$-means.
	
	In order to circumvent these, sevaral authors have published various ideas. Following are few examples:
	\begin{description}
		\item[Power method] [Boutsidis and Gittens, 2015], [Lin and Cohen,  2010]
		\item[A  careful  optimisation  of  diagonalisation  algorithms  in  the  context  of  SC]  [Liu  et  al.,  2007]
		\item[Matrix column-subsampling such as in the Nystr\"{o}m method]  (Fowlkes  et  al.,  2004)
		\item[nSPEC  and  cSPEC methods] (Wang et al., 2009), or in (Chen and Cai, 2011;Sakai and Imiya, 2009)
		\item[Reduce $k$-means complexity] Jain, 2010
		\item[Line of work on coresets] (Har-Peled and Maxmudar, 2004)
		\item[Reduction of Graph by successive aggregation of nodes] (Dhillon et al. 2007) (Filippone et al., 2008)
	\end{description}
	
	Compressive clustering circumvents two last bottlenecks using $\mathcal{O}(\log(k))$ randomly fitered signals on the graph to serve as feature vectors instead of eigenvectors and by clustering random subset of $\mathcal{O}(k\log(k))$ nodes using random feature vectors and infering the cluster label of all $N$ nodes. Thus, the complexity of $k$-means is reduced from $\mathcal{O}(Nk^{2})$ to $\mathcal{O}(k^2\log^2(k))$
	
	\chapter{Background}
	\section{Graph Signal Processing}
	\subsection*{Graph Fourier Matrix}
	Let $\mathcal{G} = (\mathcal{V},\mathcal{E}, W)$ be an undirected graph with $\mathcal{V}$ the set of $N$ nodes, $\mathcal{E}$ the set of edges and $W$ set weighted adjacency matrix of non-negative weights. The normalized Laplacian of $\mathcal{G}$ is given by $L=I-D^{1/2}WD^{-1/2}$. Here $I$ is $N\times N$ identity matrix and $D$ is a diagnoal matrix with $D_{ii} = \sum_{j\neq i}W_{ij}$. $L$ is real, symmetric and positive semi-definite, hence diagnizable as $L=U\Lambda U^T$ with orthonormal set of eigenvectors $u_1,\dots,u_N$ and eigenvalues $0=\lambda_1 \le \dots \le \lambda_N \le 2$. By analogy to continuous Laplacian operator with classical fourier modes as eigenfunctions and squared frequencies as eigenvalues, the eigenvectors are considered Fourier modes and square-roots of eigenvalues are considered frequencies of the Graph. 

	\subsection*{Graph filtering}
	The graph Fourier transform $\hat{x}$ of signal $x$ is $\hat{x} = U^Tx$. Given a continuous filter function $h$ defined on $[0,2]$, its associated graph filter operator $H\in \mathbb{R}^{N \times N}$ is defined as $H := h(L) = Uh(\Lambda)U^T$ where $h(\Lambda):=diag(h(\lambda_1), \dots, h(\lambda_N))$. The filtered signal is then $Hx$. Now, considering an ideal low-pass filter $h_{\lambda_c}$ defined as below, $H_{\lambda_c}$ is the graph filter operator associated with $h_{\lambda_c}$. 
	$$h_{\lambda_c} =  \begin{cases}1, &\text{if }\lambda \le \lambda_c \\ 0, &\text{otherwise}\end{cases}$$

	\subsection*{Fast graph filtering}
	To filter the signal by $h$ without doagonalizing $L$, we approximate $h$ by a polynomial of degree $p$ $\tilde{h}(\lambda) = \sum_{l=0}^{p}\alpha_{l}L^{l} \simeq h(\lambda), \forall \lambda \in [0,2], \alpha_1,\dots,\alpha_p \in \mathbb{R}$. Thus $\tilde{H} := \tilde{h}(L) = \sum_{l=0}^{p}\alpha_{l}L^{l} \simeq H$. Thus instead of computing dense $\tilde{H}$ we approximate $\tilde{H}x = \sum_{l=0}^{p}\alpha_{l}L^{l}x$ using successive matrix-vector products with $L$. Thus the computational complexity is $\mathcal{O}(pe)$ where $e$ is number of edges.

	\chapter{Principles of Compressive Spectral Clustering}
	\section{Ideal filtering of random signals}

	\begin{definition}[Local cumulative coherence]: 
		Given a graph $\mathcal{G}$, the local cumulative coherence of order $k$ at node $i$ is $v_k(i) = \lvert \lvert U_k^T\delta_i\rvert \rvert = \sqrt{\sum_{=1}^k U_{ij}^2}$, i.e. $\ell_2$-norm of $i{th}$ row of $U$
	\end{definition}

	Next, the authors define the diagonal matrix $V_k$ such that $V_k(i,i) = 1/v_k(i)$, assuming $v_k > 0$. Consider matrix $R = (r_1|r_2|\cdots|r_d) \in \mathbb{R}^{N\times d}$ consisting of $d$ random signals $r_i$ whose components are independent, Bernouilli, Gaussian or sparse random variables with mean zero and variance $1/d$. Considering the coherence=normalized filtered version of $R$, $V_kH_{\lambda_k}, R \in \mathbb{R}^{N\times d}$, and define node $i$'s new feature vector $f_i \in \mathbb{R}^d$ as transposed $i$-th row of the filtered matrix, i.e. $\tilde{f_i} := (V_kH_{\lambda_k}, R)^T\delta_i$. The following theorem shows that, for a large enough d, $$\tilde{D_{ij}} := \norm{\tilde{f_i} - \tilde{f_j}} = \norm{(V_kH_{\lambda_k}, R)^T(\delta_i-\delta_j)}$$ is a good estimation of $D_{ij}$ with high probability.

	\begin{theorem}
		Let $\epsilon \in [0,1]$ and $\beta > 0$ be given. If $d$ larger than $$\frac{4+2\beta}{\epsilon^2/2 - \epsilon^3/3}\log N,$$ then with probability at least $1-N^{-\beta}$, we have $$(1-\epsilon)D_{ij} \le \tilde{D}_ij \le (1+\epsilon)D_{ij}, \forall (i,j) \in \{1,\dots,N\}^2 $$
	\end{theorem}
	\begin{proof}
		To be filled in.
	\end{proof}

	\section{Downsampling and interpolation}
	Let $c_j \in \mathbb{R}^N, j=1,\dots,k$ be the indicator vectors of clusters $C_j$. The authors propose to estimate $c_j$ by running $k$-means on small subset of $n$ feature vectors only using 1) low-dimentional model that captures the regularity of $c_j$, 2) make sure enough information is preserved after sampling, 3) algorithem that rapidly and accuraely estimates the vectors $c_j$. 
	\subsection{The Low-Dimentional Model}
	For a simple regular graph (graph with nodes of same degree) with $k$ disconnected clusters, one can see that the indicator vectors ${c_i}$ form a set of $k$ orthogonal eigenvectors of $L$ with eigenvalue 0. Thus, al the indicator vectors live in $span(U_k)$. For general graph the authors assume that the indicator vectors live close to $span(U_k)$, i.e. there is a slight perturbation. In graph signal processing words, one can say that $c_j$ is approximately $k$-bandlimited. i.e. the first $k$ graph Fourier coefficients bear most of its energy. 
	\subsection{Sampling and Interpolation}
	

	\thispagestyle{fancy}
	\bibliographystyle{unsrt}
	\bibliography{long_string, Mybib} 

	\nocite{Luxburg2007, govl:96, parlet:98, stsu:90, gene:2018}

\end{document}